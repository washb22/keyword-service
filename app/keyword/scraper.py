# app/keyword/scraper.py

import time
import random
import urllib.parse
import re
import traceback
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# --- ë³´ì¡° í•¨ìˆ˜ë“¤ ---
CAFE_HOSTS = {"cafe.naver.com", "m.cafe.naver.com"}

def extract_cafe_ids(url: str):
    """ì¹´í˜ URLì—ì„œ ID ì¶”ì¶œ"""
    try: 
        p = urllib.parse.urlparse(url)
    except Exception: 
        return set()
    ids = set()
    qs = urllib.parse.parse_qs(p.query)
    for key in ("articleid", "clubid", "articleId", "clubId"):
        for val in qs.get(key, []):
            if val.isdigit(): 
                ids.add(val)
    for token in re.split(r"[/?=&]", p.path):
        if token.isdigit() and len(token) >= 4: 
            ids.add(token)
    return ids

def url_matches(target_url: str, candidate_url: str) -> bool:
    """ë‘ URLì´ ê°™ì€ ê²Œì‹œë¬¼ì¸ì§€ í™•ì¸"""
    try: 
        t, c = urllib.parse.urlparse(target_url), urllib.parse.urlparse(candidate_url)
    except Exception: 
        return False
    t_host, c_host = t.netloc.split(":")[0].lower(), c.netloc.split(":")[0].lower()
    if (t_host in CAFE_HOSTS) or (c_host in CAFE_HOSTS):
        t_ids, c_ids = extract_cafe_ids(target_url), extract_cafe_ids(candidate_url)
        if t_ids and c_ids and (t_ids & c_ids): 
            return True
        if t_ids and any(_id in candidate_url for _id in t_ids): 
            return True
    return candidate_url.startswith(target_url[: min(len(target_url), 60)])

def url_or_title_matches(target_url, target_title, candidate_link):
    """URL ë˜ëŠ” ì œëª©ìœ¼ë¡œ ë§¤ì¹­"""
    href = candidate_link.get_attribute("href") or ""
    link_text = candidate_link.text.strip()
    
    # URL ë§¤ì¹­
    if url_matches(target_url, href):
        return True
    
    # ì œëª© ë§¤ì¹­ (ê³µë°± ë“± ì •ê·œí™” í›„ ë¹„êµ)
    if target_title and link_text:
        normalized_target = "".join(target_title.split()).lower()
        normalized_link = "".join(link_text.split()).lower()
        if normalized_target in normalized_link or normalized_link in normalized_target:
            return True
    
    return False

def human_sleep(a=0.8, b=1.8):
    """ì‚¬ëŒì²˜ëŸ¼ ëœë¤ ëŒ€ê¸°"""
    time.sleep(random.uniform(a, b))

def is_valid_content_link(href):
    """'ì¼ë°˜ ì¸ê¸°ê¸€' ë¡œì§ì„ ìœ„í•œ ìœ íš¨í•œ ì½˜í…ì¸  ë§í¬ì¸ì§€ í™•ì¸"""
    if not href:
        return False
    
    exclude_patterns = [
        'javascript:', '#', '/search.naver', 'tab=', 'mode=', 'option=', 
        'query=', 'where=', 'sm=', 'ssc=', '/my.naver', 'help.naver', 
        'shopping.naver', 'terms.naver.com', 'nid.naver.com'
    ]
    href_lower = href.lower()
    if any(pattern in href_lower for pattern in exclude_patterns):
        return False
    
    include_patterns = [
        'blog.naver.com', 'cafe.naver.com', 'post.naver.com', 'kin.naver.com',
        'smartplace.naver', 'tv.naver.com', 'news.naver.com'
    ]
    if any(pattern in href for pattern in include_patterns):
        return True
    
    return False

# --- ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ---
def run_check(keyword: str, post_url: str, post_title: str = None) -> tuple:
    """í‚¤ì›Œë“œë§ˆë‹¤ ë‹¤ë¥¸ êµ¬ì¡°ë¥¼ ë™ì ìœ¼ë¡œ íŒŒì•…í•˜ì—¬ ìˆœìœ„ ì¸¡ì •"""
    print(f"--- '{keyword}' ìˆœìœ„ í™•ì¸ ì‹œì‘ ---")
    
    options = webdriver.ChromeOptions()
    options.add_argument("--headless=new")
    options.add_argument("--window-size=1280,2200")
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36")
    
    driver = None
    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        q = urllib.parse.quote(keyword)
        
        print(f"[{keyword}] í†µí•©ê²€ìƒ‰ í˜ì´ì§€ ì ‘ê·¼ ì¤‘...")
        driver.get(f"https://search.naver.com/search.naver?query={q}")
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "main_pack")))
        human_sleep()
        
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight)")
        time.sleep(1.5)

        all_sections = driver.find_elements(By.CSS_SELECTOR, ".sc_new, .view_wrap")
        print(f"[{keyword}] {len(all_sections)}ê°œ ì„¹ì…˜ ë°œê²¬")
        
        for section in all_sections:
            try:
                if not section.is_displayed() or section.size['height'] < 50:
                    continue
                
                section_title = extract_section_title(section, keyword)
                if "ì‡¼í•‘" in section_title or "ê´‘ê³ " in section_title:
                    continue

                print(f"[{keyword}] ì„¹ì…˜: '{section_title}' í™•ì¸ ì¤‘...")
                
                content_links = extract_content_links(section)
                if not content_links:
                    print(f"[{keyword}] '{section_title}'ì—ì„œ ì½˜í…ì¸  ë§í¬ë¥¼ ì°¾ì§€ ëª»í•¨")
                    continue
                
                print(f"[{keyword}] '{section_title}'ì—ì„œ {len(content_links)}ê°œ ì½˜í…ì¸  ë§í¬ ë°œê²¬")
                
                # ì¤‘ë³µ ì œê±°
                unique_links = []
                seen_hrefs = set()
                for link in content_links:
                    href = link.get_attribute('href')
                    if href not in seen_hrefs:
                        seen_hrefs.add(href)
                        unique_links.append(link)

                # ì´ ì„¹ì…˜ ë‚´ì—ì„œë§Œ ìˆœìœ„ ì¹´ìš´íŠ¸
                for rank, link in enumerate(unique_links, 1):
                    if url_or_title_matches(post_url, post_title, link):
                        print(f"âœ… [{keyword}] '{section_title}' ì„¹ì…˜ ë‚´ {rank}ìœ„ì—ì„œ ë°œê²¬!")
                        return (section_title, rank, section_title)  # ì„¹ì…˜ ë‚´ ìˆœìœ„ë§Œ ë°˜í™˜
            
            except Exception:
                continue
        
        print(f"âŒ [{keyword}] í†µí•©ê²€ìƒ‰ ê²°ê³¼ì—ì„œ URLì„ ì°¾ì§€ ëª»í•¨")
        return ("ë…¸ì¶œX", 999, None)

    except Exception as e:
        print(f"ğŸš¨ [{keyword}] ìˆœìœ„ í™•ì¸ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        traceback.print_exc()
        return ("í™•ì¸ ì‹¤íŒ¨", 999, None)
    finally:
        if driver:
            driver.quit()
        print(f"--- '{keyword}' ìˆœìœ„ í™•ì¸ ì™„ë£Œ ---\n")

def extract_section_title(section, keyword):
    """í‚¤ì›Œë“œ, í…ìŠ¤íŠ¸, í´ë˜ìŠ¤ëª… ê¸°ë°˜ìœ¼ë¡œ ì„¹ì…˜ ì œëª© ì¶”ì¶œ"""
    try:
        title_element = section.find_element(By.CSS_SELECTOR, "[class*='headline'], .title_area .title")
        if title_element and title_element.text and len(title_element.text.strip()) > 1:
            return title_element.text.strip()
            
        section_text = section.text[:200]
        if "ì¸ê¸°ê¸€" in section_text:
            match = re.search(r'([\wÂ·\s]+)?ì¸ê¸°ê¸€', section_text)
            if match:
                title = match.group(0).strip()
                if len(title) < 30: return title
            return "ì¸ê¸°ê¸€"

        class_name = section.get_attribute("class") or ""
        if "ad" in class_name or "power_link" in class_name: return "ê´‘ê³ "
        if "blog" in class_name: return "ë¸”ë¡œê·¸"
        if "cafe" in class_name: return "ì¹´í˜"

    except Exception:
        pass
    return "ê²€ìƒ‰ê²°ê³¼"

def extract_content_links(section):
    """(ìµœì¢… ê²°ì •íŒ) ìŠ¤ë§ˆíŠ¸ë¸”ë¡ì„ ë¨¼ì € ì‹œë„í•˜ê³ , ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ì¸ê¸°ê¸€ ë¡œì§ìœ¼ë¡œ ì „í™˜"""
    content_links = []
    
    # 1. ìŠ¤ë§ˆíŠ¸ë¸”ë¡ ë¡œì§ì„ ìµœìš°ì„ ìœ¼ë¡œ ì‹œë„
    try:
        post_text_containers = section.find_elements(By.CSS_SELECTOR, "div[class*='text-container']")
        if post_text_containers:
            for container in post_text_containers:
                try:
                    title_link = container.find_element(By.CSS_SELECTOR, "a[class*='text-title']")
                    content_links.append(title_link)
                except Exception:
                    continue
            if content_links:
                return content_links
    except Exception:
        pass

    # 2. ìŠ¤ë§ˆíŠ¸ë¸”ë¡ì´ ì•„ë‹ˆë¼ê³  íŒë‹¨ë˜ë©´, 'ì¼ë°˜ ì¸ê¸°ê¸€' ì›ë³¸ ë¡œì§ìœ¼ë¡œ fallback
    try:
        list_items = section.find_elements(By.CSS_SELECTOR, "li")
        if not list_items:
            # liê°€ ì—†ëŠ” ê²½ìš°, ì„¹ì…˜ ì „ì²´ì—ì„œ a íƒœê·¸ë¥¼ ì°¾ìŒ
            all_links = section.find_elements(By.TAG_NAME, "a")
            for link in all_links:
                href = link.get_attribute('href')
                if is_valid_content_link(href):
                    content_links.append(link)
        else:
            # liê°€ ìˆëŠ” ê²½ìš°, ê° li ë‚´ë¶€ì—ì„œ ìœ íš¨í•œ ë§í¬ë¥¼ ì°¾ìŒ
            for item in list_items:
                all_links_in_item = item.find_elements(By.TAG_NAME, 'a')
                for link in all_links_in_item:
                    href = link.get_attribute('href')
                    if is_valid_content_link(href):
                        content_links.append(link)
                        break # lië‹¹ í•˜ë‚˜ì˜ ìœ íš¨ ë§í¬ë§Œ ì°¾ê³  ë‹¤ìŒìœ¼ë¡œ ë„˜ì–´ê°
    except Exception:
        pass

    return content_links